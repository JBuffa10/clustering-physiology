---
title: "CART"
author: "Jacob Buffa"
date: "March 21, 2020"
output: html_document
---

## MODEL OBJECTIVE
<p> I have a subset of athltes that after training and watching for over a year, I am confident in their physiology, movement strategies, and they fit the logic derived from the Performance Driver metrics.  There is another subst of athletes that I am also confident in their movement strategies, but they are slightly off from the performance driver logic.  I am hoping to find a model that can identify a pattern that I am not recognizing to correctly classify all athletes.<p>
<br>
A bagged CART model was chosen first because there are no tuning parameters. Thought of as a 'baseline'.  Now a simple rpart classification tree is selected in order to view and analyze decision making proces of the model
<br>

### Load Libraries & Data
```{r libraries, message=FALSE, warning=FALSE}
library(caret)
library(rpart)
library(tidyverse)
library(rattle)
library(rpart.plot)

# Scoring Function
custom.scoring <- function(x){
  scale(x)*10+50
}

# Tables
athleteInfo <- read.csv("AthleteInformation1.csv")
allData <- read.csv("FINAL TABLE.csv")
```

### Data Processing
```{r process, warning=FALSE, message=FALSE}
# ORGANIZE 
athleteInfo <-
  athleteInfo %>%
  transform(Birthday = as.Date(Birthday, format = "%m/%d/%Y")) %>%
  mutate(Gender = ifelse(Gender == "F", "F","M"))

masterData <- 
  allData %>%
  mutate(Avg..Relative.Braking.Force = (Avg..Braking.Force/System.Weight)*100,
         Avg..Relative.Braking.Power = Avg..Braking.Power/System.Weight,
         Avg..Relative.Propulsive.Power = Avg..Propulsive.Power/System.Weight) %>%
  mutate(Avg..Braking.Velocity = abs(Avg..Braking.Velocity), # change negative numbers to positive
         Avg..Braking.Power = abs(Avg..Braking.Power),
         Avg..Relative.Braking.Power = abs(Avg..Relative.Braking.Power)) %>%
  transform(Date = as.Date(Date, format = "%m/%d/%Y"),
            Name = as.character(Name)) %>%
  left_join(athleteInfo, by = "Name") %>%
  filter(Gender == "M")

variables <- colnames(masterData)
brakingMetrics <- variables[c(5:7,12,13,16,17,19)]
propulsiveMetrics <- variables[c(9:11,14,15,47,53:55)]
balanceMetrics <- variables[c(25:34)]

# CENTER AND SCALE
clusterData <- masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE)
clusterData <- na.omit(clusterData) # remove NA

# IDENTIFY CORRELATED PREDICTORS
cormatrix <- cor(clusterData[,-1])
summary(cormatrix[upper.tri(cormatrix)])
#highlyCor <- findCorrelation(cormatrix, cutoff = .8) 
#filterclusterData <- clusterData[,-highlyCor]
```
Data has been cleaned, centered, and scaled for the model.  Normally would elimate linear dependencies, but Performance Driver metrics have been selected based on domain expertise
<br> 

```{r assign labels, message=FALSE, warning=FALSE}
labels <- 
  clusterData %>%
  mutate(Class = case_when(Takeoff.Velocity > Avg..Propulsive.Velocity+2 &
                             Avg..Propulsive.Force > Avg..Propulsive.Velocity+2 ~ 'Momentum',
                           Takeoff.Velocity <= Avg..Propulsive.Velocity+2 &
                             Avg..Propulsive.Force <= Avg..Propulsive.Velocity+2 &
                             Avg..Braking.Force < Avg..Braking.Velocity-2 ~ 'Power',
                           Takeoff.Velocity <= Avg..Propulsive.Velocity+2 &
                             Avg..Propulsive.Force <= Avg..Propulsive.Velocity+2 &
                             Avg..Braking.Force >= Avg..Braking.Velocity-2 ~ 'Athletic')) %>%
  select(Name, Class)

treeData <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  select(Name, Avg..Braking.Force, Avg..Braking.Velocity, Avg..Relative.Braking.Force, Avg..Propulsive.Force, 
         Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity,Avg..Propulsive.Force, Takeoff.Velocity,
         Class) %>% na.omit()
```
Condense data to assign labels to the athletes that I am confident in, and fit the logic derived from Performance Driver metrics.  Then expand data back to have more data points to train the model.  Identify Athletes that have enough tests to bias the sample, and only keep the top 40% of their tests.  Finally, remove and NA values
<br>


### Data Splitting

```{r training_testing, message=FALSE, warning=FALSE}
train.tree <- treeData[,-1] %>% filter(Class != 'Unknown')
test.tree <- treeData[,-1] %>% filter(Class == 'Unknown')
```
Ideally these would be random splits.  But for the purposes of this model, I have split the training set and testing set based on athletes with and without pre-assigned labels
<br>

### Model Training & Tuning
```{r model, message=FALSE, warning=FALSE}
# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
set.seed(234)
cart.model <- train(Class ~., data = train.tree,
                    method = 'rpart', 
                    minsplit = 0,
                    cp = 0,
                    trControl = fitControl)

cart.model
```

```{r plot model}
plot(cart.model$finalModel)
text(cart.model$finalModel)
```
<br>

### Predicting Outcomes
```{r}
# get predicted labels
cart.pred <- predict.train(cart.model, newdata = test.tree)

# Distribution of predicted labels
sum(cart.pred == 'Athletic')
sum(cart.pred == 'Power')
sum(cart.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.pred) %>% view()

```
<br>

### Conclusion
<i>This model performed better than the bagged CART model, but still not optimally.  This seemed to deem that momentum athletes could not be elite on field by setting the velocity threshold at 44 instead of taking it as a ratio.  The next step is to re-run the model, but add in a propulsive velocity difference metric.  Maybe strip away all other metrics aside from the velo difference and raw propulsive numbers.<i>
<br>
<br>

### Model 2 Training & Tuning
```{r model 2, message=FALSE, warning=FALSE}
# add velo difference as a variables
treeData.2 <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  mutate(Propulsive.Velo.Diff = Avg..Propulsive.Velocity - Takeoff.Velocity) %>%
  select(Name, Avg..Braking.Force, Avg..Braking.Velocity, Avg..Relative.Braking.Force, Avg..Propulsive.Force, 
         Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity, Takeoff.Velocity,
         Propulsive.Velo.Diff,Class) %>% na.omit()

train.tree.2 <- treeData.2[,-1] %>% filter(Class != 'Unknown')
test.tree.2 <- treeData.2[,-1] %>% filter(Class == 'Unknown')

# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
tuneGrid <- expand.grid(cp = seq(0, 0.5, 0.05))
set.seed(234)
cart.model.2 <- train(Class ~., data = train.tree.2,
                    method = 'rpart',
                    tuneGrid = tuneGrid,
                    trControl = fitControl)

cart.model.2

```


```{r plot model.2}
plot(cart.model.2$finalModel)
text(cart.model.2$finalModel)
```


<br>

### Predicting Outcomes
```{r}
# get predicted labels
cart.2.pred <- predict.train(cart.model.2, newdata = test.tree.2)

# Distribution of predicted labels
sum(cart.2.pred == 'Athletic')
sum(cart.2.pred == 'Power')
sum(cart.2.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.2.pred) %>% view()

```
<br>

### Conclusion
<i>
This model was signifacntly more accurate than the first.  The primary branch used velocity difference which was the goal.  Ideally, momentum athletes would be on the left side only, and the right side can switch back and forth between athletic and power.  This model is close but not quite there.  The next step is to remove avg. propulsive velocity. <i>
<br>
<br>

### Model 3 Training & Tuning
```{r model 3, message=FALSE, warning=FALSE}
# add velo difference as a variables
treeData.3 <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  mutate(Propulsive.Velo.Diff = Avg..Propulsive.Velocity - Takeoff.Velocity) %>%
  select(Name, Avg..Braking.Force, Avg..Braking.Velocity, Avg..Relative.Braking.Force,
         Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity, Takeoff.Velocity,
         Propulsive.Velo.Diff,Class) %>% na.omit()

train.tree.3 <- treeData.3[,-1] %>% filter(Class != 'Unknown')
test.tree.3 <- treeData.3[,-1] %>% filter(Class == 'Unknown')

# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
tuneGrid <- expand.grid(cp = seq(0, 0.5, 0.05))
set.seed(234)
cart.model.3 <- train(Class ~., data = train.tree.3,
                    method = 'rpart', 
                    tuneGrid = tuneGrid,
                    trControl = fitControl)

cart.model.3

```


```{r plot model.3}
plot(cart.model.3$finalModel)
text(cart.model.3$finalModel)
```


### Predicting Outcomes
```{r predicting model 3}
# get predicted labels
cart.3.pred <- predict.train(cart.model.3, newdata = test.tree.3)

# Distribution of predicted labels
sum(cart.3.pred == 'Athletic')
sum(cart.3.pred == 'Power')
sum(cart.3.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.3.pred) %>% view()

```

<br>

### Conclusion
<i>
This model was slightly more accurat than the second.  Stripping away Avg Propulsive Force force the model to evaluate braking metrics.  This made the left side of the tree much more accurate.  The right side needs to be addressed.  After reviewing the tree, it looks like the model may need to be trained on only high level athletes. It creates nodes to try and fit low level athletes to their label.  That is a problem with the training data. <i>
<br>
<br>
























