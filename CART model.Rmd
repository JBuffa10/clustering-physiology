---
title: "CART"
author: "Jacob Buffa"
date: "March 21, 2020"
output: html_document
---

## MODEL OBJECTIVE
<p> I have a subset of athltes that after training and watching for over a year, I am confident in their physiology, movement strategies, and they fit the logic derived from the Performance Driver metrics.  There is another subst of athletes that I am also confident in their movement strategies, but they are slightly off from the performance driver logic.  I am hoping to find a model that can identify a pattern that I am not recognizing to correctly classify all athletes.<p>
<br>
A bagged CART model was chosen first because there are no tuning parameters. Thought of as a 'baseline'.  Now a simple rpart classification tree is selected in order to view and analyze decision making proces of the model
<br>

### Load Libraries & Data
```{r libraries, message=FALSE, warning=FALSE}
library(caret)
library(rpart)
library(tidyverse)
library(rattle)
library(rpart.plot)

# Scoring Function
custom.scoring <- function(x){
  scale(x)*10+50
}

# Tables
athleteInfo <- read.csv("AthleteInformation1.csv")
allData <- read.csv("FINAL TABLE.csv")
```

### Data Processing
```{r process, warning=FALSE, message=FALSE}
# ORGANIZE 
athleteInfo <-
  athleteInfo %>%
  transform(Birthday = as.Date(Birthday, format = "%m/%d/%Y")) %>%
  mutate(Gender = ifelse(Gender == "F", "F","M"))

masterData <- 
  allData %>%
  mutate(Avg..Relative.Braking.Force = (Avg..Braking.Force/System.Weight)*100,
         Avg..Relative.Braking.Power = Avg..Braking.Power/System.Weight,
         Avg..Relative.Propulsive.Power = Avg..Propulsive.Power/System.Weight) %>%
  transform(Date = as.Date(Date, format = "%m/%d/%Y"),
            Name = as.character(Name)) %>%
  left_join(athleteInfo, by = "Name") %>%
  filter(Gender == "M")

variables <- colnames(masterData)
brakingMetrics <- variables[c(5:7,12,13,16,17,19)]
propulsiveMetrics <- variables[c(9:11,14,15,47,53:55)]
balanceMetrics <- variables[c(25:34)]

# CENTER AND SCALE
clusterData <- masterData %>% 
  mutate(Avg..Braking.Velocity = abs(Avg..Braking.Velocity), # change negative numbers to positive
         Avg..Braking.Power = abs(Avg..Braking.Power),
         Avg..Relative.Braking.Power = abs(Avg..Relative.Braking.Power)) %>%
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE)
clusterData <- na.omit(clusterData) # remove NA

# IDENTIFY CORRELATED PREDICTORS
cormatrix <- cor(clusterData[,-1])
summary(cormatrix[upper.tri(cormatrix)])
#highlyCor <- findCorrelation(cormatrix, cutoff = .8) 
#filterclusterData <- clusterData[,-highlyCor]
```
Data has been cleaned, centered, and scaled for the model.  Normally would elimate linear dependencies, but Performance Driver metrics have been selected based on domain expertise
<br> 

```{r assign labels, message=FALSE, warning=FALSE}
labels <- 
  clusterData %>%
  mutate(Class = case_when(Takeoff.Velocity > Avg..Propulsive.Velocity+2 &
                             Avg..Propulsive.Force > Avg..Propulsive.Velocity+2 ~ 'Momentum',
                           Takeoff.Velocity <= Avg..Propulsive.Velocity+2 &
                             Avg..Propulsive.Force <= Avg..Propulsive.Velocity+2 &
                             Avg..Braking.Force < Avg..Braking.Velocity-2 ~ 'Power',
                           Takeoff.Velocity <= Avg..Propulsive.Velocity+2 &
                             Avg..Propulsive.Force <= Avg..Propulsive.Velocity+2 &
                             Avg..Braking.Force >= Avg..Braking.Velocity-2 ~ 'Athletic')) %>%
  select(Name, Class)

treeData <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  select(Name, Avg..Braking.Force, Avg..Braking.Velocity, Avg..Relative.Braking.Force, Avg..Propulsive.Force, 
         Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity,Avg..Propulsive.Force, Takeoff.Velocity,
         Class) %>% na.omit()
```
Condense data to assign labels to the athletes that I am confident in, and fit the logic derived from Performance Driver metrics.  Then expand data back to have more data points to train the model.  Identify Athletes that have enough tests to bias the sample, and only keep the top 40% of their tests.  Finally, remove and NA values
<br>


### Data Splitting

```{r training_testing, message=FALSE, warning=FALSE}
train.tree <- treeData[,-1] %>% filter(Class != 'Unknown')
test.tree <- treeData[,-1] %>% filter(Class == 'Unknown')
```
Ideally these would be random splits.  But for the purposes of this model, I have split the training set and testing set based on athletes with and without pre-assigned labels
<br>

### Model Training & Tuning
```{r model, message=FALSE, warning=FALSE}
# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
set.seed(234)
cart.model <- train(Class ~., data = train.tree,
                    method = 'rpart', 
                    minsplit = 0,
                    cp = 0,
                    trControl = fitControl)

cart.model
```

```{r plot model}
plot(cart.model$finalModel)
text(cart.model$finalModel)
```
<br>

### Predicting Outcomes
```{r}
# get predicted labels
cart.pred <- predict.train(cart.model, newdata = test.tree)

# Distribution of predicted labels
sum(cart.pred == 'Athletic')
sum(cart.pred == 'Power')
sum(cart.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.pred) %>% view()

```
<br>

### Conclusion
<i>This model performed better than the bagged CART model, but still not optimally.  This seemed to deem that momentum athletes could not be elite on field by setting the velocity threshold at 44 instead of taking it as a ratio.  The next step is to re-run the model, but add in a propulsive velocity difference metric.  Maybe strip away all other metrics aside from the velo difference and raw propulsive numbers.<i>
<br>
<br>

### Model 2 Training & Tuning
```{r model 2, message=FALSE, warning=FALSE}
# add velo difference as a variables
treeData.2 <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  mutate(Propulsive.Velo.Diff = Avg..Propulsive.Velocity - Takeoff.Velocity) %>%
  select(Name, Avg..Braking.Force, Avg..Braking.Velocity, Avg..Relative.Braking.Force, Avg..Propulsive.Force, 
         Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity, Takeoff.Velocity,
         Propulsive.Velo.Diff,Class) %>% na.omit()

train.tree.2 <- treeData.2[,-1] %>% filter(Class != 'Unknown')
test.tree.2 <- treeData.2[,-1] %>% filter(Class == 'Unknown')

# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
set.seed(234)
cart.model.2 <- train(Class ~., data = train.tree.2,
                    method = 'rpart', 
                    minsplit = 0,
                    cp = 5,
                    trControl = fitControl)

cart.model.2

```


```{r plot model.2}
plot(cart.model.2$finalModel)
text(cart.model.2$finalModel)
```


<br>

### Predicting Outcomes
```{r}
# get predicted labels
cart.2.pred <- predict.train(cart.model.2, newdata = test.tree.2)

# Distribution of predicted labels
sum(cart.2.pred == 'Athletic')
sum(cart.2.pred == 'Power')
sum(cart.2.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.2.pred) %>% view()

```
<br>

### Conclusion
<i>
This model was signifacntly more accurate than the first.  The primary branch used velocity difference which was the goal.  The second branch should not use Propulsive Force, given that high momentum athletes could have high total propulsive force.  Next step is to try a 3rd model that strips away propulsive force <i>
<br>
<br>

### Model 3 Training & Tuning
```{r model 3, message=FALSE, warning=FALSE}
# add velo difference as a variables
treeData.3 <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  mutate(Propulsive.Velo.Diff = Avg..Propulsive.Velocity - Takeoff.Velocity) %>%
  select(Name, Avg..Braking.Force, Avg..Braking.Velocity, Avg..Relative.Braking.Force,
         Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity, Takeoff.Velocity,
         Propulsive.Velo.Diff,Class) %>% na.omit()

train.tree.3 <- treeData.3[,-1] %>% filter(Class != 'Unknown')
test.tree.3 <- treeData.3[,-1] %>% filter(Class == 'Unknown')

# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
set.seed(234)
cart.model.3 <- train(Class ~., data = train.tree.3,
                    method = 'rpart', 
                    minsplit = 0,
                    cp = 5,
                    trControl = fitControl)

cart.model.3

```


```{r plot model.3}
plot(cart.model.3$finalModel)
text(cart.model.3$finalModel)
```


### Predicting Outcomes
```{r predicting model 3}
# get predicted labels
cart.3.pred <- predict.train(cart.model.3, newdata = test.tree.3)

# Distribution of predicted labels
sum(cart.3.pred == 'Athletic')
sum(cart.3.pred == 'Power')
sum(cart.3.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.2.pred) %>% view()

```

<br>

### Conclusion
<i>
This model was slightly more accurat than the second.  Stripping away Avg Propulsive Force force the model to evaluate braking metrics.  I liked how it evaluated Athletic athletes on both Braking Force and Velocity.  Next Step may be to add a metric for the difference in relative propulsive vs relative braking forces <i>
<br>
<br>

### Model 4 Training & Tuning
```{r model 4, message=FALSE, warning=FALSE}
# add velo difference as a variables
treeData.4 <-
  masterData %>% 
  mutate_at(vars(brakingMetrics,propulsiveMetrics), custom.scoring) %>%
  group_by(Name, Date) %>%
  summarise_at(vars(brakingMetrics, propulsiveMetrics), mean, na.rm = TRUE) %>%
  left_join(labels, by = 'Name') %>%
  ungroup() %>%
  mutate(Class = ifelse(is.na(Class), 'Unknown',Class)) %>%
  group_by(Name) %>%
  mutate(Force.Diff = Avg..Relative.Propulsive.Force - Avg..Relative.Braking.Force) %>%
  mutate(Per = n()/1118) %>%
  mutate(Outlier = ifelse(Per > .02 & Avg..Propulsive.Force <= quantile(Avg..Propulsive.Force, .6, na.rm = TRUE),1,0)) %>%
  filter(Outlier == 0) %>%
  mutate(Propulsive.Velo.Diff = Avg..Propulsive.Velocity - Takeoff.Velocity) %>% # add relative force difference
  select(Name,Avg..Relative.Braking.Force, Avg..Relative.Propulsive.Force, Avg..Propulsive.Velocity, Takeoff.Velocity,
         Propulsive.Velo.Diff,Force.Diff, Class) %>% na.omit()

train.tree.4 <- treeData.4[,-1] %>% filter(Class != 'Unknown')
test.tree.4 <- treeData.4[,-1] %>% filter(Class == 'Unknown')

# Resampling method of repeated cross validation
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)

# Model Training
tuneGrid <- expand.grid(cp = seq(0, 0.5, 0.05))
set.seed(234)
cart.model.4 <- train(Class ~., data = train.tree.4,
                    method = 'rpart',
                    tuneGrid = tuneGrid,
                    trControl = fitControl)

cart.model.4

```


```{r plot model 4}
plot(cart.model.4$finalModel)
text(cart.model.4$finalModel)
```

### Predicting Outcomes
```{r predicting model 4}
# get predicted labels
cart.4.pred <- predict.train(cart.model.4, newdata = test.tree.4)

# Distribution of predicted labels
sum(cart.4.pred == 'Athletic')
sum(cart.4.pred == 'Power')
sum(cart.4.pred == 'Momentum')

# assign labels to athletes and view results
treeData %>%
  filter(Class == 'Unknown') %>%
  select(Name)%>%
  add_column(pred.class = cart.4.pred) %>% view()

```

<br>

### Conclusion
<i>
Seemed to have hit a cap with model 4.  The accuracy did not increase by adding the force difference metric, because it still chose the same braking metircs as model 3.  When I took those metrics away, it chose to evaluate on relative braking force.  I thought that might be better but the accuracy actually went down.  It seems maybe a more complex tree model may be needed  <i>
<br>
<br>